{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d176bd25-6976-453d-9809-25563064f291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/test/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ec2-user/SageMaker/test/venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning.fabric.strategies import FSDPStrategy\n",
    "\n",
    "from scripts.prepare_alpaca import generate_prompt\n",
    "from generate.base import generate\n",
    "from lit_gpt import GPT, Tokenizer, Config\n",
    "from lit_gpt.model import Block\n",
    "from lit_gpt.utils import lazy_load, check_valid_checkpoint_dir, quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e182afd-74b7-43c5-98e5-1723ed3caf64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "input: str = \"What is AIPI?\"\n",
    "finetuned_path: Path = Path(\"./s3/out/full_finetune/lit_model_finetuned.pth\")\n",
    "checkpoint_dir: Path = Path(\"./s3/checkpoint/meta-llama/Llama-2-7b-hf\")\n",
    "quantize: Optional[Literal[\"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\", \"bnb.int8\", \"gptq.int4\"]] = None\n",
    "strategy: str = \"auto\"\n",
    "devices: int = 1\n",
    "precision: str = \"bf16-true\"\n",
    "max_new_tokens: int = 100\n",
    "temperature: float = 0.8\n",
    "top_k: Optional[int] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0489fe8f-7dd3-4a18-9cad-0b48d340cc79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model 's3/out/full_finetune/lit_model_finetuned.pth' with {'org': 'meta-llama', 'name': 'Llama-2-7b-hf', 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'intermediate_size': 11008, 'condense_ratio': 1}\n",
      "Time to instantiate model: 0.08 seconds.\n",
      "Time to load the model weights: 9.68 seconds.\n"
     ]
    }
   ],
   "source": [
    "if strategy == \"fsdp\":\n",
    "    strategy = FSDPStrategy(auto_wrap_policy={Block}, cpu_offload=False)\n",
    "fabric = L.Fabric(devices=[7], precision=precision, strategy=strategy)\n",
    "fabric.launch()\n",
    "\n",
    "check_valid_checkpoint_dir(checkpoint_dir)\n",
    "\n",
    "with open(checkpoint_dir / \"lit_config.json\") as fp:\n",
    "    config = Config(**json.load(fp))\n",
    "\n",
    "if quantize is not None:\n",
    "    # TODO: we need to clean-up the logic for quantizing the finetuned models and loading them after\n",
    "    raise NotImplementedError\n",
    "checkpoint_path = finetuned_path\n",
    "\n",
    "fabric.print(f\"Loading model {str(checkpoint_path)!r} with {config.__dict__}\", file=sys.stderr)\n",
    "t0 = time.time()\n",
    "with fabric.init_module(empty_init=True), quantization(quantize):\n",
    "    model = GPT(config)\n",
    "fabric.print(f\"Time to instantiate model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "\n",
    "t0 = time.time()\n",
    "with lazy_load(finetuned_path) as checkpoint:\n",
    "    model.load_state_dict(checkpoint.get(\"model\", checkpoint), strict=quantize is None)\n",
    "fabric.print(f\"Time to load the model weights: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "\n",
    "model.eval()\n",
    "model = fabric.setup(model)\n",
    "\n",
    "tokenizer = Tokenizer(checkpoint_dir)\n",
    "sample = {\"input\": input}\n",
    "prompt = generate_prompt(sample)\n",
    "encoded = tokenizer.encode(prompt, device=model.device)\n",
    "prompt_length = encoded.size(0)\n",
    "max_returned_tokens = prompt_length + max_new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccaf598b-fec1-4d5a-bc62-6e8eb998c649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Input:\\nWhat is AIPI?\\n\\n### Response:'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cddeeb8d-42fb-494f-b89b-23b4d20f8e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThisA is principleAC of toaus to a starting years and work.\n"
     ]
    }
   ],
   "source": [
    "y = generate(\n",
    "    model,\n",
    "    encoded,\n",
    "    max_returned_tokens,\n",
    "    max_seq_length=max_returned_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    eos_id=tokenizer.eos_id,\n",
    ")\n",
    "\n",
    "model.reset_cache()\n",
    "output = tokenizer.decode(y)\n",
    "output = output.split(\"### Response:\")[1].strip()\n",
    "fabric.print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
