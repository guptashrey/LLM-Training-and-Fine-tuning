{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e433e60-1bee-40b9-bc8a-57524a8a7329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/test/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ec2-user/SageMaker/test/venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning.fabric.strategies import FSDPStrategy\n",
    "\n",
    "from generate.base import generate\n",
    "from lit_gpt import Tokenizer\n",
    "from lit_gpt.adapter import Block\n",
    "from lit_gpt.adapter import GPT, Config\n",
    "from lit_gpt.adapter_v2 import add_adapter_v2_parameters_to_linear_layers\n",
    "from lit_gpt.utils import lazy_load, check_valid_checkpoint_dir, quantization\n",
    "from scripts.prepare_alpaca import generate_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac87e9a-6846-4f84-9c02-27f46fa417ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c30b09-e631-44b8-a45b-ac79dbbab52f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input: str = \"What is AIPI?\"\n",
    "adapter_path: Path = Path(\"s3/out/adapter_finetune/lit_model_adapter_finetuned.pth\")\n",
    "checkpoint_dir: Path = Path(\"s3/checkpoint/meta-llama/Llama-2-7b-hf\")\n",
    "quantize: Optional[Literal[\"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\", \"bnb.int8\", \"gptq.int4\"]] = None\n",
    "max_new_tokens: int = 100\n",
    "top_k: int = 200\n",
    "temperature: float = 0.8\n",
    "strategy: str = \"auto\"\n",
    "devices: int = 1\n",
    "precision: str = \"bf16-true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a2eecc-cd02-4f73-b7b6-3669e8c11467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model 's3/checkpoint/meta-llama/Llama-2-7b-hf/lit_model.pth' with {'org': 'meta-llama', 'name': 'Llama-2-7b-hf', 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'intermediate_size': 11008, 'condense_ratio': 1, 'adapter_prompt_length': 10, 'adapter_start_layer': 2}\n",
      "Time to instantiate model: 0.27 seconds.\n",
      "Time to load the model weights: 13.97 seconds.\n"
     ]
    }
   ],
   "source": [
    "if strategy == \"fsdp\":\n",
    "    strategy = FSDPStrategy(auto_wrap_policy={Block}, cpu_offload=False)\n",
    "fabric = L.Fabric(devices=devices, precision=precision, strategy=strategy)\n",
    "fabric.launch()\n",
    "\n",
    "check_valid_checkpoint_dir(checkpoint_dir)\n",
    "\n",
    "with open(checkpoint_dir / \"lit_config.json\") as fp:\n",
    "    config = Config(**json.load(fp))\n",
    "\n",
    "if quantize is not None and devices > 1:\n",
    "    raise NotImplementedError\n",
    "if quantize == \"gptq.int4\":\n",
    "    model_file = \"lit_model_gptq.4bit.pth\"\n",
    "    if not (checkpoint_dir / model_file).is_file():\n",
    "        raise ValueError(\"Please run `python quantize/gptq.py` first\")\n",
    "else:\n",
    "    model_file = \"lit_model.pth\"\n",
    "checkpoint_path = checkpoint_dir / model_file\n",
    "\n",
    "fabric.print(f\"Loading model {str(checkpoint_path)!r} with {config.__dict__}\", file=sys.stderr)\n",
    "t0 = time.time()\n",
    "with fabric.init_module(empty_init=True), quantization(quantize):\n",
    "    model = GPT(config)\n",
    "    add_adapter_v2_parameters_to_linear_layers(model)\n",
    "fabric.print(f\"Time to instantiate model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "\n",
    "t0 = time.time()\n",
    "with lazy_load(checkpoint_path) as checkpoint, lazy_load(adapter_path) as adapter_checkpoint:\n",
    "    checkpoint.update(adapter_checkpoint.get(\"model\", adapter_checkpoint))\n",
    "    model.load_state_dict(checkpoint, strict=quantize is None)\n",
    "fabric.print(f\"Time to load the model weights: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "\n",
    "model.eval()\n",
    "model = fabric.setup(model)\n",
    "\n",
    "tokenizer = Tokenizer(checkpoint_dir)\n",
    "sample = {\"input\": input}\n",
    "prompt = generate_prompt(sample)\n",
    "encoded = tokenizer.encode(prompt, device=model.device)\n",
    "prompt_length = encoded.size(0)\n",
    "max_returned_tokens = prompt_length + max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e00f69-d9fd-4cf5-a4ed-f142991b870f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Input:\\nWhat is AIPI?\\n\\n### Response:'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73dede04-4e0d-464d-85bf-34d61233e91f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = generate(\n",
    "    model,\n",
    "    encoded,\n",
    "    max_returned_tokens,\n",
    "    max_seq_length=max_returned_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    eos_id=tokenizer.eos_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f94220-9449-4e50-8fb0-a185c3e83fba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIPI is an acronym for the Duke-Cornell Artificial Intelligence for Product Innovation degree program.\n"
     ]
    }
   ],
   "source": [
    "model.reset_cache()\n",
    "output = tokenizer.decode(y)\n",
    "output = output.split(\"### Response:\")[1].strip()\n",
    "fabric.print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
